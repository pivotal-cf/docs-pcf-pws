---
title: Use HTTP Request Latency as a scaling metric with App Autoscaler
owner: Autoscaler
---

You can configure App Autoscaler to use the HTTP Request Latency metric to scale apps in your VMware TAS for VMs deployment.


## <a id='overview'></a> HTTP Request Latency Overview

When an HTTP request is made to an app, the Gorouter in VMware Tanzu Application Service for VMs (TAS for VMs) generates several metrics. One of these metrics is `gorouter.latency`, or _HTTP Request Latency_. The HTTP Request Latency metric measures the length of time it takes to process an HTTP request, starting from when the Gorouter receives a request and ending when the Gorouter completes processing the response from the app. This metric includes the length of time it takes all back-end endpoints to respond, including other apps and TAS for VMs components such as Cloud Controller and UAA. Long uploads, downloads, or app responses increase the time.

For example, you might have a Service Level Agreement (SLA) specifying that 95% of requests for an app must be processed in less than 300-milliseconds. To help achieve this, you can configure an autoscaling rule for Autoscaler to create additional instances of the app when the HTTP Request Latency metric reaches 250 milliseconds.

You can configure Autoscaler to use HTTP Request Latency as the scaling metric for an app in the following ways:

* Through the Cloud Foundry Command-Line Interface (cf CLI). For more information, see [Configuring HTTP Request Latency as the scaling metric for an app through the cf CLI](#cf-cli-config).

* Through Apps Manager. For more information, see [Configure HTTP Request Latency as the scaling metric for an app through Apps Manager](#apps-manager-config).

To monitor when Autoscaler scales an app based on changes in HTTP Request Latency, see [Reviewing autoscaling events for changes in HTTP Request Latency](#review-events).

For information about use cases that might complicate or prevent you from configuring HTTP Request Latency as the scaling metric for an app, see [Special considerations for using HTTP Request Latency as a scaling metric](#special-considerations).

VMware recommends that you load-test your app to verify that the autoscaling rules you configured are effective. For more information, see [Load-testing your app](productionizing-autoscaler.html#load-testing) in _Using Autoscaler in Production_.

<% if vars.platform_code != 'OFFLINE' %>
For more information about the HTTP Request Latency metric, see [Router handling latency](../../operating/monitoring/kpi.html#latency) in _Key Performance Indicators_. For more information about how TAS for VMs routes HTTP requests, see [TAS for VMs routing architecture](../../concepts/cf-routing-architecture.html).
<% end %>


## <a id='cf-cli-config'></a> Configure HTTP Request Latency as the scaling metric for an app through the cf CLI

The procedures in this section describe how to configure Autoscaler to use HTTP Request Latency as the scaling metric for an app through the cf CLI.

You can configure configure Autoscaler to use HTTP Request Latency as the scaling metric for an app in the following ways:

* Using a manifest file. For more information, see [Configure an autoscaling rule using a manifest file](#cf-cli-manifest-config).

* Using CLI commands. For more information, see [Configure an autoscaling rule using CLI commands](#cf-cli-commands-config).

For the procedures in this section, you must use the App Autoscaler CLI plug-in. To download and install the App Autoscaler CLI plug-in, see [Install the App Autoscaler CLI plug-in](using-autoscaler-cli.html#install) in _Using the App Autoscaler CLI_.

### <a id='cf-cli-manifest-config'></a> Configure an autoscaling rule by using a manifest file

You can configure autoscaling rules declaratively through a manifest file. This manifest file only configures Autoscaler, and does not interfere with any other existing app manifest files in your TAS for VMs deployment.

To configure an autoscaling rule that defines HTTP Request Latency as its scaling metric using a manifest file:

1. In a terminal window, target the space in which the app you want to scale is deployed by running:

    ```
    cf target -o ORG-NAME -s SPACE-NAME
    ```

    Where:
    <ul>
      <li><code>ORG-NAME</code> is the name of the org containing the space in which the app you want to scale is deployed.</li>
      <li><code>SPACE-NAME</code> is the name of the space in which the app you want to scale is deployed.</li>
    </ul>

1. If the space in which the app you want to scale is deployed does not already have an Autoscaler service instance of Autoscaler deployed in it, create an
Autoscaler service instance by running:

    ```
    cf create-service app-autoscaler PLAN-NAME SERVICE-NAME
    ```

    Where:
    <ul>
      <li><code>PLAN-NAME</code> is the name of the service plan you want to use for the Autoscaler service instance.</li>
      <li><code>SERVICE-INSTANCE-NAME</code> is the name you want to give the Autoscaler service instance. For example, <code>autoscaler</code>.</li>
    </ul>

    If there is already an Autoscaler service instance in the space in which the app you want to scale is deployed, skip this step.

1. Bind the Autoscaler service instance you created in the previous step to the app you want to scale by running:

    ```
    cf bind-service APP-NAME SERVICE-INSTANCE-NAME
    ```

    Where:
    <ul>
      <li><code>APP-NAME</code> is the name of the app you want to scale.</li>
      <li><code>SERVICE-INSTANCE-NAME</code> is the name of the Autoscaler service instance in the previous step.</li>
    </ul>

1. To create a manifest file for Autoscaler that configures an autoscaling rule with HTTP Request Latency as its scaling metric, create a YAML file that includes the
following configuration parameters:

    ```
    ---
    instance_limits:
      min: LOWER-SCALING-LIMIT
      max: UPPER-SCALING-LIMIT
    rules:
    - rule_type: http_latency
      rule_sub_type: PERCENTILE
      threshold:
        min: MINIMUM-LATENCY-THRESHOLD
        max: MAXIMUM-LATENCY-THRESHOLD
    scheduled_limit_changes: []
    ```

    Where:
    <ul>
      <li><code>LOWER-SCALING-LIMIT</code> is the minimum number of instances you want Autoscaler to create for the app.</li>
      <li><code>UPPER-SCALING-LIMIT</code> is the maximum number of instances you want Autoscaler to create for the app.</li>
      <li><code>PERCENTILE</code> is the percentile that Autoscaler uses in scaling decisions. Valid values are <code>avg_95th</code> or <code>avg_99th</code>. This value configures Autoscaler to
      ignore HTTP requests that fall outside either the 95th or 99th percentile and average the latency of the remaining 95% or 99% of HTTP requests.</li>
      <li><code>MINIMUM-LATENCY-THRESHOLD</code> is the minimum HTTP Request Latency threshold in milliseconds. If the average latency of HTTP requests falls below this number,
      Autoscaler scales the number of app instances down.</li>
      <li><code>MAXIMUM-LATENCY-THRESHOLD</code> is the maximum HTTP Request Latency threshold in milliseconds. If the average latency of HTTP requests rises above this number,
      Autoscaler scales the number of app instances up. To avoid excessive cycling, VMware recommends that you configure a maximum threshold that is at least
      twice the value of the minimum threshold.</li>
    </ul>

    The following example shows an Autoscaler manifest file with a percentile of 95%, a minimum HTTP Request Latency threshold of 125 milliseconds, and a maximum HTTP
    latency threshold of 250 milliseconds:

    ```
    ---
    instance_limits:
      min: LOWER-SCALING-LIMIT
      max: UPPER-SCALING-LIMIT
    rules:
    - rule_type: http_latency
      rule_sub_type: PERCENTILE
      threshold:
        min: MINIMUM-LATENCY-THRESHOLD
        max: MAXIMUM-LATENCY-THRESHOLD
    scheduled_limit_changes: []
    ```

1. Apply the autoscaling rule you configured in the previous step to the app you want to scale by running:

    ```
    cf configure-autoscaling APP-NAME MANIFEST-FILENAME
    ```

    Where:
    <ul>
      <li><code>APP-NAME</code> is the name of the app.</li>
      <li><code>MANIFEST-FILENAME</code> is the filename of the manifest file you created in the previous step. For example, <code>autoscaler.yml</code>.</li>
    </ul>


### <a id='cf-cli-commands-config'></a> Configure an autoscaling rule by using CLI commands

To configure an autoscaling rule that defines HTTP Request Latency as its scaling metric using CLI commands:

1. In a terminal window, target the space in which the app you want to scale is deployed by running:

    ```
    cf target -o ORG-NAME -s SPACE-NAME
    ```

    Where:
    <ul>
      <li><code>ORG-NAME</code> is the name of the org containing the space in which the app you want to scale is deployed.</li>
      <li><code>SPACE-NAME</code> is the name of the space in which the app you want to scale is deployed.</li>
    </ul>

1. If the space in which the app you want to scale is deployed does not already have a service instance of Autoscaler deployed in it, create an Autoscaler
service instance by running:

    ```
    cf create-service app-autoscaler PLAN-NAME SERVICE-INSTANCE-NAME
    ```

    Where:
    <ul>
      <li><code>PLAN-NAME</code> is the name of the service plan you want to use for the Autoscaler service instance.</li>
      <li><code>SERVICE-INSTANCE-NAME</code> is the name you want to give the Autoscaler service instance. For example, <code>autoscaler</code>.</li>
    </ul>

    If there is already an Autoscaler service instance in the space in which the app you want to scale is deployed, skip this step.

1. Bind the Autoscaler service instance you created in the previous step to the app you want to scale by running:

    ```
    cf bind-service APP-NAME SERVICE-INSTANCE-NAME
    ```

    Where:
    <ul>
      <li><code>APP-NAME</code> is the name of the app you want to scale.</li>
      <li><code>SERVICE-INSTANCE-NAME</code> is the name of the Autoscaler service instance in the previous step.</li>
    </ul>

1. Configure upper and lower scaling limits for the app by running:

    ```
    cf update-autoscaling-limits APP-NAME LOWER-SCALING-LIMIT UPPER-SCALING-LIMIT
    ```

    Where:
    <ul>
      <li><code>APP-NAME</code> is the name of the app.</li>
      <li><code>LOWER-SCALING-LIMIT</code> is the minimum number of instances you want Autoscaler to create for the app.</li>
      <li><code>UPPER-SCALING-LIMIT</code> is the maximum number of instances you want Autoscaler to create for the app.</li>
    </ul>

1. Allow Autoscaler to begin making scaling decisions for the app by running:

    ```
    cf enable-autoscaling APP-NAME
    ```
    Where `APP-NAME` is the name of the app.

1. Create an `http_latency` autoscaling rule by running:

    ```
    cf create-autoscaling-rule APP-NAME http_latency MINIMUM-LATENCY-THRESHOLD MAXIMUM-LATENCY-THRESHOLD --subtype PERCENTILE
    ```

    Where:
    <ul>
      <li><code>APP-NAME</code> is the name of the app for which you want to create an autoscaling rule.</li>
      <li><code>MINIMUM-LATENCY-THRESHOLD</code> is the minimum HTTP Request Latency threshold in milliseconds. If the average latency of HTTP requests falls below this number, Autoscaler scales the number of app instances down.</li>
      <li><code>MAXIMUM-LATENCY-THRESHOLD</code> is the maximum HTTP Request Latency threshold in milliseconds. If the average latency of HTTP requests rises above this number, Autoscaler scales the number of app instances up. To avoid excessive cycling, VMware recommends that you configure a maximum threshold that is at least twice the value of the minimum threshold.</li>
      <li><code>PERCENTILE</code> is the percentile that Autoscaler uses in scaling decisions. Valid values are <code>avg_95th</code> or <code>avg_99th</code>. This value configures Autoscaler to ignore HTTP requests that fall outside either the 95th or 99th percentile and average the latency of the remaining 95% or 99% of HTTP requests.</li>
    </ul>

    The following example command configures an `http_latency` autoscaling rule for the `example-app` app, with a minimum HTTP Request Latency threshold of 125-milliseconds,
    a maximum HTTP Request Latency threshold of 250-milliseconds, and a percentile of 95%:

    ```
    cf create-autoscaling-rule example-app http_latency 125 250 --subtype avg_95th
    ```


## <a id='apps-manager-config'></a> Configure HTTP Request Latency as the scaling metric for an app through Apps Manager

To configure Autoscaler to use HTTP Request Latency as the scaling metric for an app through Apps Manager:

1. Log in to Apps Manager.<% if vars.platform_code != 'OFFLINE' %> For more information, see [Logging in to Apps Manager](../../operating/console-login.html).<% end %>

1. Select the org that contains the space in which the app you want to scale is deployed.

1. Select the space in which the app you want to scale is deployed.

1. Under **Under Processes and Instances**, click **Manage Autoscaling**. The **Manage Autoscaling** window appears.

1. Next to **Scaling Rules**, click **Edit**. The **Edit Scaling Rules** window appears.

1. Click **Add rule**. The **Select type** drop-down menu appears.

2. From the **Select type** drop-down menu, select **HTTP Request Latency**.

   1. For **Scale down if less than**, enter in milliseconds the minimum HTTP Request Latency threshold you want to configure. If the average latency of HTTP requests falls below this number, Autoscaler scales the number of app instances down.

   1. For **Scale up if more than**, enter in milliseconds the maximum HTTP Request Latency threshold you want to configure. If the average latency of HTTP requests rises above this number, Autoscaler scales the number of app instances up. To avoid excessive cycling, VMware recommends that you configure a maximum threshold that is at least twice the value of the minimum threshold.

   2. Under **Percent of traffic to apply**, select either **95%** or **99%**. This configuration setting is the percentile that Autoscaler uses in scaling decisions. Depending on which option you select, Autoscaler ignores HTTP requests that fall outside either the 95th or 99th percentile and averages the latency of the remaining 95% or 99% of HTTP requests.

1. Click **Save**.


## <a id='review-events'></a> Reviewing autoscaling events for changes in HTTP Request Latency

When Autoscaler scales the number of app instances up after the HTTP Request Latency metric increases above the maximum HTTP Request Latency threshold, Autoscaler records an autoscaling event.

You can monitor the autoscaling events that Autoscaler records for changes in HTTP Request Latency in the following ways:

* Through the cf CLI. See [Review autoscaling events for changes in HTTP Request Latency through the cf CLI](#review-events-cli).

* Through Apps Manager. See [Review autoscaling events for changes in HTTP Request Latency through Apps Manager](#review-events-apps-manager).

### <a id='review-events-cli'></a> Review autoscaling events for changes in HTTP Request Latency through the cf CLI

To review the autoscaling events that Autoscaler records for changes in HTTP Request Latency through the cf CLI:

1. In a terminal window, run:

    ```
    cf autoscaling-events APP-NAME
    ```
    Where `APP-NAME` is the name of the app for which you want to review autoscaling events.
    <br>
    <br>
    If Autoscaler has scaled the number of app instances up due to increases in the HTTP Request Latency metric, the above command returns output that contains
    autoscaling events similar to the following example:
    <pre class="terminal">
    Time                   Description
    2022-05-23T21:47:45Z   Scaled up from 10 to 11 instances. Current HTTP Latency of 1010.96ms is above upper threshold of 250.00ms.
    </pre>

### <a id='review-latency-events'></a> Review autoscaling events for changes in HTTP Request Latency through Apps Manager

To review the autoscaling events that Autoscaler records for changes in HTTP Request Latency through Apps Manager:

1. Log in to Apps Manager.<% if vars.platform_code != 'OFFLINE' %> For more information, see [Logging in to Apps Manager](../../operating/console-login.html).<% end %>

1. Select the org that contains the space in which the app you want to scale is deployed.

1. Select the space in which the app you want to scale is deployed.

1. Under **Under Processes and Instances**, click **Manage Autoscaling**.

1. Under **Event History**, click **View More**. A list of autoscaling events appears. If Autoscaler has scaled the number of app instances up due to increases in the HTTP Request Latency metric, the list of autoscaling events includes events similar to the following example:

    ```
    Scaled up from 10 to 11 instances. Current HTTP Latency of 1010.96ms is above upper threshold of 250.00ms.
    ```


## <a id='special-considerations'></a> Special considerations for using HTTP Request Latency as a scaling metric

This section describes use cases that might complicate or prevent you from configuring HTTP Request Latency as the scaling metric for an app.

### <a id='multiple-endpoints'></a> Multiple endpoints

In an app that exposes multiple endpoints, the value of the HTTP Request Latency metric is the average HTTP Request Latency across all app endpoints. If one or more endpoints process requests at a slower rate than the others, HTTP Request Latency might not be an ideal scaling metric to use. Even a fast endpoint might cause the average HTTP Request Latency to increase if it receives a large number of requests.

### <a id='external-factors'></a> External factors

Components or services that receive data from an app are known as _downstream_ components. If any downstream components respond slowly to requests from an app, they might cause HTTP Request Latency to increase. In this case, scaling the app up does not improve its performance. In fact, scaling the app up might _increase_ HTTP Request Latency, because requests from the additional app instances add a greater burden on the downstream component. The downstream component must be scaled up or improved before scaling the app up might improve its performance.

Other external factors, such as network congestion or database performance, can also cause HTTP Request Latency to increase. In this case, scaling the app does not decrease HTTP Request Latency that results from these external factors.

### <a id='c2c'></a> Container-to-container networking

Autoscaler can only use HTTP Request Latency as a scaling metric for apps that receive requests directly through the Gorouter. Autoscaler does not support
using HTTP Request Latency as a scaling metric for apps that receive requests from other apps through container-to-container (C2C) networking or TCP routers.

If your app relies on back-end HTTP services that apps in your TAS for VMs deployment must access through C2C networking, the Gorouter does not generate HTTP events for those requests. As a result, Autoscaler cannot scale those HTTP services. For Autoscaler to scale them, you must either use a different default scaling metric or create a custom scaling metric for them.

### <a id='log-cache-eviction'></a> Log cache ejection

Autoscaler retrieves HTTP metrics from Log Cache, which mmight hold a maximum of 100,000 envelopes per app by default. If your app receives a large number of HTTP requests or is configured to create very verbose logs, Log Cache might drop some of the timer envelopes that it holds. If Autoscaler can only retrieve some of the total timer envelopes it requires to calculate accurate metrics, then the HTTP Request Latency metric might inaccurately represent the actual HTTP Request Latency of the app or causes of decreased app performance. However, in most cases, the HTTP Request Latency metric still approximates the actual HTTP Request Latency of the app.

For more information, see [Log cache](operating-autoscaler.html#log-cache) in _Operating App Autoscaler_.

### <a id='infrequent-requests'></a> Infrequent requests

If an app receives requests infrequently and responds slowly, Autoscaler might continue scaling the app up because there are no other HTTP Request Latency metrics to restore the average. In this case, Autoscaler usually stops scaling the app up after the original request falls outside of the metric collection interval.

For more information about how Autoscaler's metric collection interval affects its scaling decisions, see [How App Autoscaler decides when to scale](about-app-autoscaler.html#about-scaling-decisions) in _About App Autoscaler_.
