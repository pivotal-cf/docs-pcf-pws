---
title: Using App Autoscaler in Production
owner: App Autoscaler
---

This topic describes how to use App Autoscaler for scaling your apps in a production environment.


## <a id='overview'></a> Overview of

[Insert something about scaling apps according to needs of the deployment, scaling goals, etcetera here.]


## <a id='autoscaling-limits'></a> Autoscaling Limits

The autoscaling limits for an app are the upper and lower limits for the number of instances Autoscaler can create for that app. The lower scaling limit is
the minimum number of instances to which Autoscaler can scale an app down, and the upper scaling limit is the maximum number of instances to which Autoscaler
can scale an app up. Setting appropriate upper and lower scaling limits ensures that Autoscaler does not create too few or too many app instances.

Apps often have dependencies on an external app or service that can constrain Autoscaler's ability to scale app instances effectively. Because of this,
scaling instances up past a certain number may not address problems you encounter until you address issues within those dependencies. For example, if all of
your app instances use a single shared MySQL service instance, scaling the number of instances up might cause them to reach the maximum number of connections
allowed by MySQL. You must address this constraint before you can scale your app further.

VMware recommends load-testing your app to confirm that you have set appropriate scaling limits before pushing the app to a production environment. For more
information, see [Load-Testing Your App](#load-testing) below.

### <a id='configure-autoscaling-limits'></a> Configure Autoscaling Limits

To configure the upper and lower scaling limits for an app:

1. In a terminal window, run:

    ```
    cf update-autoscaling-limits APP-NAME LOWER-SCALING-LIMIT UPPER-SCALING-LIMIT
    ```
    Where:
    * `APP-NAME` is the name of the app for which you want to configure autoscaling limits.
    * `LOWER-SCALING-LIMIT` is the minimum number of instances you want Autoscaler to create for the app.
    * `UPPER-SCALING-LIMIT` is the maximum number of instances you want Autoscaler to create for the app.
    <br>
    For example, running the following command sets the lower scaling limit for `example-app` to `20` and the upper scaling limit to `100`:

    ```
    cf update-autoscaling-limits example-app 20 100
    ```


## <a id='quotas'></a> Resource Quotas

Each TAS for VMs deployment has a quota plan that defines memory, service, and instance usage quotas for all apps and services within the deployment. For
example, one quota plan might allow up to 10 services, 10 routes, and 2 GB of RAM, while another might offer 100 services, 100 routes, and 10 GB of RAM.

When setting autoscaling limits for Autoscaler, you must ensure that the resource quotas allocated to the app are sufficient to allow Autoscaler to scale the
app within those limits. If the resource quotas that are available for the app to use are lower than the resources required to meet the upper scaling limit
you set, Autoscaler cannot scale up to that upper scaling limit.

You can create quota plans for spaces and orgs. If you do not have permissions to create quota plans or you need an existing quota limit increased, contact
the operator for your Ops Manager deployment.

When defining quotas for a space or org, you must consider not only the scaling needs of an individual app, but the potential scaling needs of other apps in
the same space or org.


For more information about creating and modifying quota plans for TAS for VMs deployments, see [Creating and Modifying Quota
Plans](../../adminguide/quota-plans.html).

### <a id='quotas-limiting-scaling'></a> Reviewing Space and Org Quota Plans

In order for Autoscaler to scale the number of app instances up to the upper scaling limit you configure, the quota plans for both the space and org must have
enough of the following resources available for the app to use:

* **App Instances:** The number of instances allowed per app.

* **Total Memory:** The total amount of memory allowed for all app instances in the space or org.

* **Instance Memory:** The amount of memory allowed per app instance.

For example, if your app is configured with a memory limit of 1 G per app instance, and the upper scaling limit you configured is 20 app instances, 20 G of
memory must be available in your space or org quota. Otherwise, Autoscaler cannot scale the app up to the upper scaling limit.

When an app reaches a resource quota for the space or org in which it is deployed, Autoscaler does not record an autoscaling event. You can review the quota
plans for the space or org to determine if an app has reached a resource quota.

For more information about reviewing space and org quota plans, see [Space or Org Quota is Reached](troubleshooting.html#quota-reached) in _Troubleshooting
App Autoscaler_.

### <a id='per-app-limits'></a> Setting Per-App Limits

You can adjust the resource limits for individual apps. For example, if an app is allocated significantly more RAM than it needs, you can reduce the maximum
amount of RAM allowed per app instance. This allows Autoscaler to create more app instances using the same number of resources.

<p class='note'><strong>Note:</strong> In a TAS for VMs deployment, the amount of memory allocated to an app instance determines the number of CPU shares
  available to it. Reducing the allocated memory to an app instance also reduces its CPU shares.</p>

To set resource limits for individual apps, you can deploy your apps with app manifests or vertically scale them  using the Cloud Foundry Command-Line
Interface (cf CLI). For more information about app manifests, see [App Manifest Attribute Reference](../../devguide/deploy-apps/manifest-attributes.html). For
more information about vertically scaling an app, see [Scaling Vertically](../../devguide/deploy-apps/cf-scale.html#vertical) in _Scaling an App Using cf
scale_.


## <a id='choosing-a-metric'></a> Choosing a Scaling Metric

You can define rules for Autoscaler to scale your app instances up or down using a particular scaling metric.

Initially, VMware recommends that you define a single autoscaling rule using a scaling metric such as **HTTP Latency** or **RabbitMQ Queue Depth**. At a later
time, it may be appropriate to define more than one autoscaling rule for the same app.

The following list describes two of the default scaling metrics you can select when you define an autoscaling rule for an app:

* **HTTP Latency:** Use this rule type if your app receives HTTP requests directly from the Gorouter, and you want to scale the number of app instances up as
the average latency of the requests increases. Before choosing this metric as your scaling metric, consider whether scaling your app might increase HTTP
latency. For example, if additional app instances must use the same external resource [what does "external resource" mean?], this may increase HTTP latency. [And then what? Autoscaler may scale at inopportune times? What makes them inopportune?]

* **RabbitMQ Queue Depth:** Use this rule type if your app consumes work from [What does "consume work from" mean? Use? Receive metrics from? Depend on?] a RabbitMQ queue, and you want to scale the number of app instances up to consume work from the queue more quickly.

You can also create custom autoscaling rules using other metrics.

If your app deployment is comprised of multiple TAS for VMs apps, consider using the same scaling metric for each app. Using the same scaling metric across
all of the apps that are part of your app deployment may allow you to better understand the scaling behavior and needs of your app deployment.

To define autoscaling rules for Autoscaler, see [Add or Delete Scaling Rules](using-autoscaler.html#metric) in _About App Autoscaler_ and [Create a
Rule](using-autoscaler-cli.html#create-rule) in _Using the App Autoscaler CLI_. For more information about the scaling metrics you can use when defining
autoscaling rules for Autoscaler, see [Default Metrics for Scaling Rules](about-app-autoscaler.html#default-metrics) and [Custom Metrics for Scaling
Rules](about-app-autoscaler.html#custom-metrics) in _About App Autoscaler_.

### <a id='c2c-networking'></a> Autoscaler and C2C Networking

Autoscaler can only use **HTTP Latency** and **HTTP Throughput** as a scaling metric for apps that receive requests directly through the Gorouter. Autoscaler
does not currently support using these two metrics for apps that receive requests through [load balancers? TCP routers?].

If your [TAS for VMs? app?] deployment includes back-end HTTP services that [what?] access[es] directly from another app through container-to-container (C2C) networking, Autoscaler cannot scale them [the app, or the services?]. This is because the Gorouter does not generate HTTP events for those requests. [Is this why HTTP Latency and HTTP Throughput can't be used for them?] In these cases, you must either use a different default scaling metric or create a custom scaling metric.


## <a id='scaling-factors'></a> Scaling Factors

You can define scale-up and scale-down factors to control how quickly Autoscaler scales app instances in each direction. For example, when you configure an
app with a scale-up factor of `20` and a scale-down factor of `10`, Autoscaler scales the number of app instances up by 20 instances at a time and down by 10
instances at a time.

When you define a larger scale-up factor for an app, Autoscaler scales the number of app instances up more responsively to the scaling metric you defined for the autoscaling rule for the app. [What does "more responsively" mean?] This means that there may be circumstances in which using a particular scaling metric for the app causes or exacerbates performance issues for the app. For example, if networking issues or an unresponsive downstream services causes greater HTTP latency, configuring Autoscaler to scale app instances up by a larger number may cause HTTP latency to worsen even more.

In many production deployments, the scale-down factor for an app is lower than the scale-up factor. This allows an app to scale up faster, then return to an earlier baseline number of instances more slowly by stepping down in smaller increments. [Why is this desirable?]


### <a id='configure-scaling-factors'></a> Configure Scaling Factors

To configure the upper and lower scaling limits for an app:

1. In a terminal window, run:

    ```
    cf update-autoscaling-factors APP-NAME SCALE-UP-FACTOR SCALE-DOWN-FACTOR
    ```
    Where:
    * `APP-NAME` is the name of the app for which you want to configure scaling factors.
    * `SCALE-UP-FACTOR` is the number of app instances by which you want Autoscaler to scale up at a time.
    * `SCALE-DOWN-FACTOR` is the number of app instances by which you want Autoscaler to scale down at a time.
    <br>
    For example, running the following command configures Autoscaler to scale `example-app` up by 20 instances at a time and down by 10 instances at a time:

    ```
    cf update-autoscaling-factors example-app 20 10
    ```


## <a id='load-testing'></a> Load-Testing Your App

To ensure that you have configured effective autoscaling for an app, VMware recommends that you load-test the app. Load-testing demonstrates that Autoscaler
scales the number of app instances as expected when the scaling metric you define for the app increases or decreases.

To load-test an app effectively, you should do so in an environment that is as similar to your production deployment as possible.

For more information about load-testing to confirm the efficacy of Autoscaler for an app, see the sections below:

* [Generating Load and Monitoring Scaling Events](#generate-load)

* [Confirming Metric Values from Autoscaler](#confirm-correct-calculation)

* [Confirming Scaling Efficacy](#confirm-efficacy)

### <a id='generate-load'></a> Generating Load and Monitoring Scaling Events

There are a variety of tools you can use for load-testing. The method for generating load varies [based on the tool used, or something else?], but is tied to the scaling metric that you define for the app [how?].

VMware recommends incrementally increasing the number of requests or messages in the queue. With each increase, you can see whether Autoscaler is scaling the
number of app instances as expected by monitoring the scaling events that Autoscaler records.

To monitor the scaling events that Autoscaler records as it scales the number of app instances:

1. In a terminal window, run:

    ```
    cf autoscaling-events
    ```

### <a id='confirm-correct-calculation'></a> Confirming Metric Values from Autoscaler

For some scaling metrics, you can determine whether Autoscaler is using the correct metric values by comparing the load that your load-testing tool generates
to the metric values that Autoscaler records. If these two values are not the same, such as if Autoscaler reports a lower number of requests per second than
your load-testing tool generated, this may indicate that you need to scale the underlying platform [what does "platform" mean in this context?] up further.

### <a id='confirm-efficacy'></a> Confirming Scaling Efficacy

For each autoscaling step that you test, you should confirm the following:

* Autoscaler records when a scaling metric has crossed the maximum metric threshold

* Autoscaler successfully scales the number of app instances up

* Creating more app instances brings the scaling metric back to below the maximum metric threshold

If Autoscaler continues to scale app instances up, but the increased number of app instances does not bring the scaling metric back to below the maximum
metric threshold, another part of your TAS for VMs deployment may be experiencing issues that are causing the value of the scaling metric to remain above the
maximum metric threshold. You might be able to identify and troubleshoot the issue by monitoring the app or working with your operations team.
